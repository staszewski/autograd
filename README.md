# Autograd

A minimal implementation of automatic differentiation from scratch. Learn how modern deep learning frameworks compute gradients automatically.

## ğŸš€ Features

- **Addition**: `a + b`
- **Subtraction**: `a - b` 
- **Multiplication**: `a * b`
- **Automatic Gradients**: Computes derivatives via backpropagation
- **Scalar & Tensor Operations**: Works with both `tensor + scalar` and `scalar + tensor`

## ğŸ“– Quick Start

```python
from autograd import Tensor

# Create tensors with gradient tracking
x = Tensor(2.0, requires_grad=True)
y = Tensor(3.0, requires_grad=True)

# Build expression: z = x * y + x
temp = x * y  # 6.0
z = temp + x  # 8.0

# Compute gradients automatically
z.backward()

print(f"âˆ‚z/âˆ‚x = {x._grad}")  # 4.0 (y + 1)
print(f"âˆ‚z/âˆ‚y = {y._grad}")  # 2.0 (x)
```

## ğŸ§® Math Behind the Magic

### Basic Derivatives
```
âˆ‚(a + b)/âˆ‚a = 1,  âˆ‚(a + b)/âˆ‚b = 1
âˆ‚(a - b)/âˆ‚a = 1,  âˆ‚(a - b)/âˆ‚b = -1  
âˆ‚(a * b)/âˆ‚a = b,  âˆ‚(a * b)/âˆ‚b = a
```

### Chain Rule
For nested functions `z = f(g(x))`:
```
âˆ‚z/âˆ‚x = âˆ‚z/âˆ‚g Ã— âˆ‚g/âˆ‚x
```

### Gradient Accumulation
When a variable appears multiple times, gradients add:
```python
# z = x * y + x * 2
# âˆ‚z/âˆ‚x = y + 2  (sum of all paths)
```

## ğŸ§ª Testing

```bash
# Run all tests
uv run pytest

# Run specific tests
uv run pytest tests/test_mul.py -v
```

## ğŸ—ï¸ Architecture

```
autograd/
â”œâ”€â”€ tensor.py      # Tensor class with gradient tracking
â”œâ”€â”€ context.py     # Saves data for backward pass  
â”œâ”€â”€ operation.py   # Abstract operation base class
â””â”€â”€ arithmetic.py  # Add, Sub, Mul implementations
```

Built with clean OOP principles for educational purposes.
```